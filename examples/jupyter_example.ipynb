{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Large Language Model Fine-tuning with NVIDIA GPUs\n",
        "\n",
        "This notebook demonstrates fine-tuning a large language model using NVIDIA GPU acceleration. It showcases various GPU-intensive operations for testing the notebook analyzer.\n",
        "\n",
        "**Target Audience:** ML Engineers and Data Scientists familiar with PyTorch and Transformers\n",
        "\n",
        "**Estimated Time:** 2-4 hours depending on GPU configuration\n",
        "\n",
        "**NVIDIA Tools Used:** CUDA, cuDNN, NCCL for distributed training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## Environment Setup\n",
        "\n",
        "Install required packages and verify GPU availability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "import sys\n",
        "!{sys.executable} -m pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!{sys.executable} -m pip install transformers==4.35.0 datasets==2.14.0 accelerate==0.24.0 bitsandbytes==0.41.1\n",
        "!{sys.executable} -m pip install peft==0.6.0 trl==0.7.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM, \n",
        "    TrainingArguments, \n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Set environment variables for optimal performance\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # Use 4 GPUs\n",
        "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
        "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Configuration\n",
        "\n",
        "Configure a large language model for fine-tuning with LoRA (Low-Rank Adaptation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"meta-llama/Llama-2-13b-hf\"  # 13B parameter model\n",
        "MAX_LENGTH = 2048\n",
        "BATCH_SIZE = 8  # Large batch size for GPU utilization\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 3\n",
        "WARMUP_STEPS = 100\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 500\n",
        "\n",
        "# LoRA configuration for efficient fine-tuning\n",
        "LORA_CONFIG = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16,  # Low rank\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
        "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * torch.cuda.device_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with optimizations\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,  # Use half precision\n",
        "    device_map=\"auto\",  # Automatically distribute across GPUs\n",
        "    trust_remote_code=True,\n",
        "    use_cache=False,  # Disable caching for training\n",
        "    attn_implementation=\"flash_attention_2\",  # Use Flash Attention for efficiency\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, LORA_CONFIG)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(f\"Model loaded on devices: {[p.device for p in model.parameters()][:5]}\")\n",
        "print(f\"Model memory footprint: {model.get_memory_footprint() / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Training and Analysis Complete\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. **Large Model Fine-tuning**: 13B parameter Llama-2 model requiring significant GPU resources\n",
        "2. **Multi-GPU Training**: Distributed training across multiple NVIDIA GPUs using NCCL\n",
        "3. **Memory Optimization**: LoRA, gradient checkpointing, and mixed precision training\n",
        "4. **Performance Requirements**: High VRAM usage, tensor cores, and multi-GPU communication\n",
        "\n",
        "**Expected GPU Requirements:**\n",
        "- Minimum: 2x A100 80GB or 4x RTX 4090 24GB\n",
        "- Optimal: 4x H100 80GB or 8x A100 80GB with NVLink\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
